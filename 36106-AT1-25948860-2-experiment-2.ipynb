{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6wc2HE0pke"
      },
      "source": [
        "# **Experiment Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVpE17Ahezu"
      },
      "source": [
        "---\n",
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S8jFaNXqvV5W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] 拒绝访问。: 'C:\\\\Users\\\\brohao\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\~2learn\\\\.libs\\\\msvcp140.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You can now save your data files in: c:\\Users\\brohao\\Desktop\\UTS\\36106\\AT1\\36106\\assignment\\AT1\\data\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "!pip install -q utstd\n",
        "\n",
        "from utstd.folders import *\n",
        "from utstd.ipyrenders import *\n",
        "\n",
        "at = AtFolder(\n",
        "    course_code=36106,\n",
        "    assignment=\"AT1\",\n",
        ")\n",
        "at.run()\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNUpX8nW8PzT"
      },
      "source": [
        "---\n",
        "## Student Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M3sAurDs8QgT"
      },
      "outputs": [],
      "source": [
        "student_name = \"Jiayu Hao\"\n",
        "student_id = \"25948860\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JPhiKXcc8QpA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">student_name</p><h1 font-size: 3em>Jiayu Hao</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_name', value=student_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "frhpHjw_8RHw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">student_id</p><h1 font-size: 3em>25948860</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_id', value=student_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOR4jtDCmeT"
      },
      "source": [
        "---\n",
        "## 0. Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgTrMfyylVLf"
      },
      "source": [
        "### 0.a Install Additional Packages\n",
        "\n",
        "> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D79tb2V-lVpJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXFKfa2tp1ch"
      },
      "source": [
        "### 0.b Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GBEAwdncnlAx"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "import pandas as pd\n",
        "import altair as alt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q1Bzcejvfpm"
      },
      "source": [
        "---\n",
        "## A. Experiment Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2kA9PB4YNcOc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">experiment_id</p><h1 font-size: 3em>2</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "experiment_id = \"2\"\n",
        "print_tile(size=\"h1\", key='experiment_id', value=experiment_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JLRy6rZpNcQe"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_hypothesis = \"\"\"\n",
        "The hypothesis is that Lasso with L1 regularization can keep accuracy while performing automatic feature selection, making it more robust than Ridge when redundant or correlated features exist. \n",
        "The question is whether different alpha values can lower validation MAE and at the same time reduce the number of active features, improving interpretability and reducing noise.\n",
        "It is worthwhile because insurance data often has many correlated or low-value features, and Lasso can shrink or remove them. \n",
        "This creates a simpler and more interpretable model, reduces noise, improves stability.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0MGyBOfHNcSk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">experiment_hypothesis</p><h3 font-size: 3em>\n",
              "The hypothesis is that Lasso with L1 regularization can keep accuracy while performing automatic feature selection, making it more robust than Ridge when redundant or correlated features exist. \n",
              "The question is whether different alpha values can lower validation MAE and at the same time reduce the number of active features, improving interpretability and reducing noise.\n",
              "It is worthwhile because insurance data often has many correlated or low-value features, and Lasso can shrink or remove them. \n",
              "This creates a simpler and more interpretable model, reduces noise, improves stability.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='experiment_hypothesis', value=experiment_hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tzO3uh5gNcUu"
      },
      "outputs": [],
      "source": [
        "# Detail what will be the expected outcome of the experiment. If possible, estimate the goal you are expecting.\n",
        "# List the possible scenarios resulting from this experiment.\n",
        "experiment_expectations = \"\"\"\n",
        "The expected outcome is that Lasso will match or improve validation MAE compared to Ridge (~127) while reducing the number of active features. \n",
        "This should give a simpler and more stable model, lower overfitting risk, and provide a clear list of key drivers for premium pricing. \n",
        "If MAE improves, Lasso can be adopted; if similar, it still adds value by improving interpretability; if worse, alpha can be retuned or other models considered.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OjiYRSpXNcZW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">experiment_expectations</p><h3 font-size: 3em>\n",
              "The expected outcome is that Lasso will match or improve validation MAE compared to Ridge (~127) while reducing the number of active features. \n",
              "This should give a simpler and more stable model, lower overfitting risk, and provide a clear list of key drivers for premium pricing. \n",
              "If MAE improves, Lasso can be adopted; if similar, it still adds value by improving interpretability; if worse, alpha can be retuned or other models considered.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='experiment_expectations', value=experiment_expectations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NCwQQFkU3v5"
      },
      "source": [
        "---\n",
        "## B. Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NKgOzSn-w0eq"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "# Load data\n",
        "try:\n",
        "  X_train = pd.read_csv(at.folder_path / 'X_train.csv')\n",
        "  y_train = pd.read_csv(at.folder_path / 'y_train.csv')\n",
        "\n",
        "  X_val = pd.read_csv(at.folder_path / 'X_val.csv')\n",
        "  y_val = pd.read_csv(at.folder_path / 'y_val.csv')\n",
        "\n",
        "  X_test = pd.read_csv(at.folder_path / 'X_test.csv')\n",
        "  y_test = pd.read_csv(at.folder_path / 'y_test.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zfC-DLKv4AuM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 34\n"
          ]
        }
      ],
      "source": [
        "features_list = list(X_train.columns)\n",
        "print(\"Number of features:\", len(features_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ba0he6GRNqw6"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_selection_explanations = \"\"\"\n",
        "The selected features include numerical variables such as customer seniority, vehicle attributes, and claim history, as well as one-hot encoded categorical variables like gender, policy type, and channel. \n",
        "Identifiers such as ID, name, address, phone, and email were removed because they do not contribute to prediction. \n",
        "Only features with direct or indirect impact on premiums were kept.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BBgXyumuNtJw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">feature_selection_explanations</p><h3 font-size: 3em>\n",
              "The selected features include numerical variables such as customer seniority, vehicle attributes, and claim history, as well as one-hot encoded categorical variables like gender, policy type, and channel. \n",
              "Identifiers such as ID, name, address, phone, and email were removed because they do not contribute to prediction. \n",
              "Only features with direct or indirect impact on premiums were kept.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_selection_explanations', value=feature_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8MNBrC4Zgz6"
      },
      "source": [
        "---\n",
        "## C. Train Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_tQitOfeDXr"
      },
      "source": [
        "### C.1 Import Algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9zGCsesGNxTN"
      },
      "outputs": [],
      "source": [
        "\n",
        "algorithm_selection_explanations = \"\"\"\n",
        "Lasso is a good fit because L1 regularization can shrink unimportant feature coefficients to zero, reducing dimensionality and improving interpretability. \n",
        "It also handles multicollinearity by selecting only part of a correlated group, making results easier to explain.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iEkT3HqoNxVp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">algorithm_selection_explanations</p><h3 font-size: 3em>\n",
              "Lasso is a good fit because L1 regularization can shrink unimportant feature coefficients to zero, reducing dimensionality and improving interpretability. \n",
              "It also handles multicollinearity by selecting only part of a correlated group, making results easier to explain.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='algorithm_selection_explanations', value=algorithm_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ks_MmM2mCfm"
      },
      "source": [
        "### C.2 Set Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y1MqjyPnOL6H"
      },
      "outputs": [],
      "source": [
        "# Set Hyperparameters\n",
        "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "# Set Hyperparameters through the Result\n",
        "alphas_new = [0.07, 0.08, 0.09, 0.1, 0.11]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xD8dH3eUNz33"
      },
      "outputs": [],
      "source": [
        "\n",
        "hyperparameters_selection_explanations = \"\"\"\n",
        "We tune alpha because it controls the strength of regularization. \n",
        "A larger alpha makes the model sparser but may underfit, while a smaller alpha is closer to linear regression and may overfit. \n",
        "We test a range of values (0.0001 to 10) and select the best on the validation set.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CSPbGOKvNz6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">hyperparameters_selection_explanations</p><h3 font-size: 3em>\n",
              "We tune alpha because it controls the strength of regularization. \n",
              "A larger alpha makes the model sparser but may underfit, while a smaller alpha is closer to linear regression and may overfit. \n",
              "We test a range of values (0.0001 to 10) and select the best on the validation set.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='hyperparameters_selection_explanations', value=hyperparameters_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDjdjQjFmkLe"
      },
      "source": [
        "### C.3 Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0Ub3Nrdgmm2N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso Regression Results:\n",
            "alpha | Train MAE | Validation MAE | #Nonzero Coefs\n",
            "0.0001 |     34.67 |         127.56 |            30\n",
            "0.001 |     34.68 |         127.54 |            30\n",
            " 0.01 |     34.68 |         127.36 |            26\n",
            "  0.1 |     34.76 |         126.54 |            22\n",
            "    1 |     35.34 |         130.14 |            15\n",
            "   10 |     38.09 |         147.72 |             0\n",
            "\n",
            "Best alpha by Validation MAE: 0.1\n",
            "Best MAE (Train/Val): 34.76 / 126.54\n",
            "Nonzero Coefs: 22\n"
          ]
        }
      ],
      "source": [
        "lasso_results = []\n",
        "coef_cards = []  # Record Non-zero coefs\n",
        "\n",
        "for a in alphas:\n",
        "    lasso = Lasso(alpha=a, random_state=42, max_iter=10000)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    \n",
        "    y_tr_pred = lasso.predict(X_train)\n",
        "    y_va_pred = lasso.predict(X_val)\n",
        "    \n",
        "    tr_mae = mean_absolute_error(y_train, y_tr_pred)\n",
        "    va_mae = mean_absolute_error(y_val,   y_va_pred)\n",
        "    nonzero = int(np.sum(lasso.coef_ != 0))\n",
        "    \n",
        "    lasso_results.append((a, tr_mae, va_mae, nonzero))\n",
        "    coef_cards.append((a, nonzero))\n",
        "\n",
        "print(\"Lasso Regression Results:\")\n",
        "print(\"alpha | Train MAE | Validation MAE | #Nonzero Coefs\")\n",
        "for a, tr, va, nnz in lasso_results:\n",
        "    print(f\"{a:5} | {tr:9.2f} | {va:14.2f} | {nnz:13d}\")\n",
        "\n",
        "# Choose best alpha by Validation MAE\n",
        "best_alpha, best_train_mae, best_val_mae, best_nnz = sorted(lasso_results, key=lambda x: x[2])[0]\n",
        "print(\"\\nBest alpha by Validation MAE:\", best_alpha)\n",
        "print(f\"Best MAE (Train/Val): {best_train_mae:.2f} / {best_val_mae:.2f}\")\n",
        "print(\"Nonzero Coefs:\", best_nnz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Lasso Regression Results ===\n",
            "alpha | Train MAE | Validation MAE | Nonzero Coefs\n",
            " 0.07 |     34.74 |         126.51 |            23\n",
            " 0.08 |     34.75 |         126.46 |            23\n",
            " 0.09 |     34.76 |         126.50 |            23\n",
            "  0.1 |     34.76 |         126.54 |            22\n",
            " 0.11 |     34.77 |         126.58 |            22\n",
            "\n",
            "Best alpha by Validation MAE: 0.08\n",
            "Best MAE (Train/Val): 34.75 / 126.46\n",
            "Nonzero Coefs: 23\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lasso_results = []\n",
        "coef_cards = []  # Record Non-zero coefs\n",
        "\n",
        "for a in alphas_new:\n",
        "    lasso = Lasso(alpha=a, random_state=42, max_iter=10000)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    \n",
        "    y_tr_pred = lasso.predict(X_train)\n",
        "    y_va_pred = lasso.predict(X_val)\n",
        "    \n",
        "    tr_mae = mean_absolute_error(y_train, y_tr_pred)\n",
        "    va_mae = mean_absolute_error(y_val,   y_va_pred)\n",
        "    nonzero = int(np.sum(lasso.coef_ != 0))\n",
        "    \n",
        "    lasso_results.append((a, tr_mae, va_mae, nonzero))\n",
        "    coef_cards.append((a, nonzero))\n",
        "\n",
        "print(\"=== Lasso Regression Results ===\")\n",
        "print(\"alpha | Train MAE | Validation MAE | Nonzero Coefs\")\n",
        "for a, tr, va, nnz in lasso_results:\n",
        "    print(f\"{a:5} | {tr:9.2f} | {va:14.2f} | {nnz:13d}\")\n",
        "\n",
        "# Choose best alpha by Validation MAE\n",
        "best_alpha, best_train_mae, best_val_mae, best_nnz = sorted(lasso_results, key=lambda x: x[2])[0]\n",
        "print(\"\\nBest alpha by Validation MAE:\", best_alpha)\n",
        "print(f\"Best MAE (Train/Val): {best_train_mae:.2f} / {best_val_mae:.2f}\")\n",
        "print(\"Nonzero Coefs:\", best_nnz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coef shape: (34,)\n",
            "X_train shape: (32136, 34)\n",
            "payment_method_0                      -1.610867e+01\n",
            "second_driver_0                       -1.224851e+01\n",
            "distribution_channel_1                 7.092442e+00\n",
            "vehicle_value                          6.371612e+00\n",
            "policy_type_3                         -5.709206e+00\n",
            "total_claims_number_ratio              3.935519e+00\n",
            "car_age                               -3.595484e+00\n",
            "lapsed_policies                        3.290664e+00\n",
            "driving_experience                    -3.145398e+00\n",
            "total_claims_number_in_history         2.896067e+00\n",
            "seniority                             -2.518055e+00\n",
            "vehicle_weight                         2.167951e+00\n",
            "current_policies_held                 -2.141609e+00\n",
            "vehicle_length                         1.823253e+00\n",
            "total_claims_number_in_current_year   -1.638586e+00\n",
            "vehicle_horsepower                     1.235289e+00\n",
            "gender_m                               8.560614e-01\n",
            "vehicle_fuel_type_D                   -8.033245e-01\n",
            "vehicle_cylinder                      -6.283746e-01\n",
            "max_policies_held                     -4.702714e-01\n",
            "total_claims_cost_in_current_year     -2.830757e-02\n",
            "payment_method_1                       6.965024e-14\n",
            "second_driver_1                        1.210726e-15\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Fit\n",
        "best = Lasso(alpha=0.08, random_state=42, max_iter=5000).fit(X_train, y_train)\n",
        "\n",
        "# Check shape\n",
        "print(\"coef shape:\", best.coef_.shape)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "\n",
        "# Use Column name of X_train\n",
        "coef_series = pd.Series(best.coef_, index=X_train.columns)\n",
        "\n",
        "# Print 23 Non-zero features\n",
        "print(coef_series[coef_series != 0].sort_values(key=abs, ascending=False).head(23))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Q4izZ7-CFk"
      },
      "source": [
        "---\n",
        "## D. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q43YtqpdeniY"
      },
      "source": [
        "### D.1 Model Technical Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4OfznSDMN4cY"
      },
      "outputs": [],
      "source": [
        "# Provide some explanations on model performance\n",
        "model_performance_explanations = \"\"\"\n",
        "The model performance is read by tracking validation MAE across alpha values. \n",
        "Small alpha gives low train MAE but high validation MAE, showing overfitting. \n",
        "As alpha increases, validation MAE may drop to a minimum, while the number of nonzero coefficients falls, showing feature sparsity. \n",
        "The best Lasso model is compared with Ridge and the baseline: \n",
        "if validation MAE is lower, L1 feature selection adds value; if not, more nonlinear or interaction features may be needed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-3F53tNBN4kZ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">model_performance_explanations</p><h3 font-size: 3em>\n",
              "The model performance is read by tracking validation MAE across alpha values. \n",
              "Small alpha gives low train MAE but high validation MAE, showing overfitting. \n",
              "As alpha increases, validation MAE may drop to a minimum, while the number of nonzero coefficients falls, showing feature sparsity. \n",
              "The best Lasso model is compared with Ridge and the baseline: \n",
              "if validation MAE is lower, L1 feature selection adds value; if not, more nonlinear or interaction features may be needed.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='model_performance_explanations', value=model_performance_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1HgZMPcmtu7"
      },
      "source": [
        "### D.2 Business Impact from Current Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T532tsbKN7ln"
      },
      "outputs": [],
      "source": [
        "# Interpret the results of the experiments related to the business objective set earlier. Estimate the impacts of the incorrect results for the business (some results may have more impact compared to others)\n",
        "business_impacts_explanations = \"\"\"\n",
        "Lower MAE means smaller pricing errors, which supports fairer premiums and reduces both customer loss from overpricing and claim risk from underpricing. \n",
        "Lasso also provides a clear list of key factors through nonzero coefficients, helping the pricing team understand which variables matter most for premiums and guiding risk control and product strategy. \n",
        "If some features are dropped, it shows they may add noise or instability, and the business can review whether to improve or stop collecting them.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ad-r_nx-N7oy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">business_impacts_explanations</p><h3 font-size: 3em>\n",
              "Lower MAE means smaller pricing errors, which supports fairer premiums and reduces both customer loss from overpricing and claim risk from underpricing. \n",
              "Lasso also provides a clear list of key factors through nonzero coefficients, helping the pricing team understand which variables matter most for premiums and guiding risk control and product strategy. \n",
              "If some features are dropped, it shows they may add noise or instability, and the business can review whether to improve or stop collecting them.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='business_impacts_explanations', value=business_impacts_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp1Ie9o8nDl1"
      },
      "source": [
        "## E. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fcRVgWd6N_TY"
      },
      "outputs": [],
      "source": [
        "# <Student to fill this section and then remove this comment>\n",
        "experiment_outcome = \"Hypothesis Partially Confirmed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4eiYtkt0N_V8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">experiment_outcomes_explanations</p><h2 font-size: 3em>Hypothesis Partially Confirmed</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h2\", key='experiment_outcomes_explanations', value=experiment_outcome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6RG2fnxTN_YI"
      },
      "outputs": [],
      "source": [
        "# <Student to fill this section and then remove this comment>\n",
        "experiment_results_explanations = \"\"\"\n",
        "The Lasso experiment shows that L1 regularization can both lower validation MAE (126.5 vs. 148 baseline) and perform automatic feature selection, leaving 22 nonzero coefficients that give a clear list of drivers for premiums. \n",
        "The result improves interpretability, which still can be improved, so further experiments are worthwhile. \n",
        "The next steps are: \n",
        "(1) feature engineering with interactions and nonlinear terms,; \n",
        "(2) ElasticNet to balance sparsity and stability; \n",
        "(3) alternative models such as tree-based methods; \n",
        "(4) data quality improvements; \n",
        "(5) segmented modeling by policy or fuel type. \n",
        "Experiments should continue following the ranked priorities.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qbXjp06qN_an"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">experiment_results_explanations</p><h2 font-size: 3em>\n",
              "The Lasso experiment shows that L1 regularization can both lower validation MAE (126.5 vs. 148 baseline) and perform automatic feature selection, leaving 22 nonzero coefficients that give a clear list of drivers for premiums. \n",
              "The result improves interpretability, which still can be improved, so further experiments are worthwhile. \n",
              "The next steps are: \n",
              "(1) feature engineering with interactions and nonlinear terms,; \n",
              "(2) ElasticNet to balance sparsity and stability; \n",
              "(3) alternative models such as tree-based methods; \n",
              "(4) data quality improvements; \n",
              "(5) segmented modeling by policy or fuel type. \n",
              "Experiments should continue following the ranked priorities.\n",
              "</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h2\", key='experiment_results_explanations', value=experiment_results_explanations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
